{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from typing import Any,Callable,Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_config = {\n",
    "    \"user\" :\n",
    "        {\"num_layers\" : 8, \"embed_dim\"  : 192, \"mlp_dim\" : 192*4, \"num_heads\" : 8},\n",
    "    \"base\" :\n",
    "        {\"num_layers\" : 12, \"embed_dim\"  : 768, \"mlp_dim\" : 3072, \"num_heads\" : 12},\n",
    "    \"large\" :\n",
    "        {\"num_layers\" : 24, \"embed_dim\"  : 1024, \"mlp_dim\" : 4096, \"num_heads\" : 16},\n",
    "    \"huge\" :\n",
    "        {\"num_layers\" : 32, \"embed_dim\"  : 1280, \"mlp_dim\" : 5120, \"num_heads\" : 16}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patch2Vector(nn.Module):\n",
    "    def __init__ (self, patch_size, channel, embed_dim, num_patches, embed_type = 'conv', ):\n",
    "        super().__init__()\n",
    "        if embed_type =='conv':\n",
    "            self.projection = nn.Sequential(\n",
    "                nn.Conv2d(channel, embed_dim, kernel_size=patch_size, stride=patch_size),\n",
    "                Rearrange('batch (embed_dim) h w -> batch (h w) embed_dim')\n",
    "            )\n",
    "        elif embed_type == 'flatten': #proposed method in original VIT\n",
    "            self.projection = nn.Sequential(\n",
    "                Rearrange('batch c (h p1) (w p2) -> batch (h w) (p1 p2)', p1 = patch_size, p2 = patch_size),\n",
    "                nn.Linear(patch_size**2*channel, embed_dim),\n",
    "            )\n",
    "        else :\n",
    "            raise NotImplementedError(\"embed_type only 'conv' or flat \")\n",
    "        self.cls_token= nn.Parameter(torch.randn(1, 1, embed_dim)) #shape (1, 1, embed_dim)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(num_patches + 1, embed_dim)) #shape (num_patches+1, embed_dim)\n",
    "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
    "        x = self.projection(x)# b,c,h,w -> b (h*w)/p^2 c*p^2 : batch num_patches embed_dim\n",
    "        batch = x.shape[0]\n",
    "        x_cls = repeat(self.cls_token, '1 1 e -> b 1 e', b=batch) \n",
    "        embedding = torch.cat((x_cls, x), dim=1) + self.pos_embedding # b n+1 e. add cls_token \n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadSelfAtteintion(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, drop_out):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.scaling = (embed_dim//num_heads)**-0.5\n",
    "        \n",
    "        self.qkv = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim*3), \n",
    "            Rearrange('b n (qkv d) -> qkv b n d' , qkv = 3),\n",
    "            Rearrange('qkv b n (h d) -> qkv b h n d', h = num_heads)\n",
    "            )\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.o = nn.Linear(embed_dim, embed_dim)\n",
    "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
    "        q, k, v= self.qkv(x) #b h n d\n",
    "        \n",
    "        att_score = torch.matmul(q,k.transpose(-2,-1))*self.scaling # [b h (n d)]@[b h (d n)] -> b h n n, @ is mat mul  \n",
    "        att_score = torch.softmax(att_score, dim = -1)\n",
    "        att_score = self.dropout(att_score)\n",
    "        \n",
    "        att = torch.matmul(att_score, v) # [b h (n n)] @ [b h (n d)] -> b h n d\n",
    "        att = rearrange(att, 'b h n d -> b n (h d)')\n",
    "        \n",
    "        return self.o(att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, mlp_dim, drop_out):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_out),\n",
    "            nn.Linear(mlp_dim,embed_dim),\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.ModuleList):\n",
    "    def __init__(self, embed_dim, mlp_dim, num_heads, drop_out):\n",
    "        super().__init__()\n",
    "        self.MSA = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            MultiheadSelfAtteintion(embed_dim, num_heads, drop_out),\n",
    "            )\n",
    "        self.FF = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            FeedForward(embed_dim, mlp_dim, drop_out),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.MSA(x) + x\n",
    "        x = self.dropout(x)\n",
    "        x = self.FF(x) + x\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class \n",
    "class VIT(nn.Module):\n",
    "    def __init__(self,  image_size : Tuple[int, int, int] = (3, 224, 224), patch_size : int = 16, num_classes : int = 1000, embed_type :str = 'conv', vit_type :str = 'base' ,pred_type : str = 'mean', dropout = 0., embed_dropout = 0.):\n",
    "        super().__init__()\n",
    "        assert vit_type in ('user','base', 'large', 'huge'), f\"vit_type must be 'usser' or 'base' or 'large' or 'huge'. but {vit_type}\"\n",
    "        assert pred_type in ('mean', 'cls_token'), f\"pred_type must be 'mean' or 'cls_token'. but {pred_type}\"\n",
    "        self.pred_type = pred_type\n",
    "        self.num_classes = num_classes\n",
    "        self.channel , self.height,self.width = image_size\n",
    "        assert not (self.height%patch_size or self.width%patch_size)  , f\"image size must be divisible by patch size,({self.height},{self.width}) can't devide by {patch_size} \"\n",
    "        self.num_patches = (self.height*self.width)//patch_size**2\n",
    "        self.patch_size = patch_size\n",
    "        self.config = vit_config[vit_type]\n",
    "        for k, v in self.config.items(): setattr(self, k, v)\n",
    "        \n",
    "        self.att_dropout_ratio = dropout\n",
    "        self.embed_dropout_ratio = embed_dropout\n",
    "        \n",
    "        assert self.embed_dim==self.patch_size**2*self.channel, f\"embed dimension of VIT-{vit_type} : {self.embed_dim}. but {self.patch_size**2*self.channel} \" \n",
    "        ###patch embedding\n",
    "        self.patch2vec = Patch2Vector(patch_size,self.channel, self.embed_dim, self.num_patches, embed_type)\n",
    "        self.embed_dropout = nn.Dropout(embed_dropout)\n",
    "        ###Tansformer Encoder\n",
    "        self.encoders = nn.ModuleList([EncoderBlock(self.embed_dim, self.mlp_dim, self.num_heads, dropout) for _ in range(self.num_layers)])\n",
    "        ###classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(self.embed_dim),\n",
    "            nn.Linear(self.embed_dim, num_classes))\n",
    "    def forward(self,x : torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x : (batch, channel, height, width)\n",
    "        \"\"\"\n",
    "        if len(x.shape)==4:\n",
    "            pass\n",
    "        elif len(x.shape)==3:\n",
    "            x = x.unsqueeze(dim=0)\n",
    "        else :\n",
    "            raise ValueError(f\"input dimension only allowed by (batch, channel, height, width) or (channel, height, width) but {x.shape}\")\n",
    "        assert x.shape[-2]==self.height and x.shape[-1]==self.width, f\"expected height and width are {(self.height, self.width)} but {x.shape}\"\n",
    "        x = self.patch2vec(x) \n",
    "        x = self.embed_dropout(x) #batch num_patches+1 embed_dim\n",
    "        \n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)+x#batch num_patches+1 embed_dim\n",
    "        x = x.mean(dim=1) if self.pred_type=='mean' else x[:,0] # b n+1 e -> b e\n",
    "        #class token을 이용해 예측하는 경우 b n e 중 n의 첫번째에 해당하는 벡터이므로 이를 이용, 평균을 이용하는 경우 n에 대한 평균\n",
    "        '''\n",
    "        Both during pre-training and fine-tuning, a classification head is attached to z^0_L\n",
    "        The classification head is implemented by a MLP with one hidden layer at pre-training\n",
    "        time and by a single linear layer at fine-tuning time.\n",
    "        '''\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 192, 4, 4]          37,056\n",
      "         Rearrange-2              [-1, 16, 192]               0\n",
      "      Patch2Vector-3              [-1, 17, 192]               0\n",
      "           Dropout-4              [-1, 17, 192]               0\n",
      "         LayerNorm-5              [-1, 17, 192]             384\n",
      "            Linear-6              [-1, 17, 576]         111,168\n",
      "         Rearrange-7           [-1, 2, 17, 192]               0\n",
      "         Rearrange-8         [-1, 2, 8, 17, 24]               0\n",
      "           Dropout-9            [-1, 8, 17, 17]               0\n",
      "           Linear-10              [-1, 17, 192]          37,056\n",
      "MultiheadSelfAtteintion-11              [-1, 17, 192]               0\n",
      "          Dropout-12              [-1, 17, 192]               0\n",
      "        LayerNorm-13              [-1, 17, 192]             384\n",
      "           Linear-14              [-1, 17, 768]         148,224\n",
      "             GELU-15              [-1, 17, 768]               0\n",
      "          Dropout-16              [-1, 17, 768]               0\n",
      "           Linear-17              [-1, 17, 192]         147,648\n",
      "      FeedForward-18              [-1, 17, 192]               0\n",
      "          Dropout-19              [-1, 17, 192]               0\n",
      "        LayerNorm-20              [-1, 17, 192]             384\n",
      "           Linear-21              [-1, 17, 576]         111,168\n",
      "        Rearrange-22           [-1, 2, 17, 192]               0\n",
      "        Rearrange-23         [-1, 2, 8, 17, 24]               0\n",
      "          Dropout-24            [-1, 8, 17, 17]               0\n",
      "           Linear-25              [-1, 17, 192]          37,056\n",
      "MultiheadSelfAtteintion-26              [-1, 17, 192]               0\n",
      "          Dropout-27              [-1, 17, 192]               0\n",
      "        LayerNorm-28              [-1, 17, 192]             384\n",
      "           Linear-29              [-1, 17, 768]         148,224\n",
      "             GELU-30              [-1, 17, 768]               0\n",
      "          Dropout-31              [-1, 17, 768]               0\n",
      "           Linear-32              [-1, 17, 192]         147,648\n",
      "      FeedForward-33              [-1, 17, 192]               0\n",
      "          Dropout-34              [-1, 17, 192]               0\n",
      "        LayerNorm-35              [-1, 17, 192]             384\n",
      "           Linear-36              [-1, 17, 576]         111,168\n",
      "        Rearrange-37           [-1, 2, 17, 192]               0\n",
      "        Rearrange-38         [-1, 2, 8, 17, 24]               0\n",
      "          Dropout-39            [-1, 8, 17, 17]               0\n",
      "           Linear-40              [-1, 17, 192]          37,056\n",
      "MultiheadSelfAtteintion-41              [-1, 17, 192]               0\n",
      "          Dropout-42              [-1, 17, 192]               0\n",
      "        LayerNorm-43              [-1, 17, 192]             384\n",
      "           Linear-44              [-1, 17, 768]         148,224\n",
      "             GELU-45              [-1, 17, 768]               0\n",
      "          Dropout-46              [-1, 17, 768]               0\n",
      "           Linear-47              [-1, 17, 192]         147,648\n",
      "      FeedForward-48              [-1, 17, 192]               0\n",
      "          Dropout-49              [-1, 17, 192]               0\n",
      "        LayerNorm-50              [-1, 17, 192]             384\n",
      "           Linear-51              [-1, 17, 576]         111,168\n",
      "        Rearrange-52           [-1, 2, 17, 192]               0\n",
      "        Rearrange-53         [-1, 2, 8, 17, 24]               0\n",
      "          Dropout-54            [-1, 8, 17, 17]               0\n",
      "           Linear-55              [-1, 17, 192]          37,056\n",
      "MultiheadSelfAtteintion-56              [-1, 17, 192]               0\n",
      "          Dropout-57              [-1, 17, 192]               0\n",
      "        LayerNorm-58              [-1, 17, 192]             384\n",
      "           Linear-59              [-1, 17, 768]         148,224\n",
      "             GELU-60              [-1, 17, 768]               0\n",
      "          Dropout-61              [-1, 17, 768]               0\n",
      "           Linear-62              [-1, 17, 192]         147,648\n",
      "      FeedForward-63              [-1, 17, 192]               0\n",
      "          Dropout-64              [-1, 17, 192]               0\n",
      "        LayerNorm-65              [-1, 17, 192]             384\n",
      "           Linear-66              [-1, 17, 576]         111,168\n",
      "        Rearrange-67           [-1, 2, 17, 192]               0\n",
      "        Rearrange-68         [-1, 2, 8, 17, 24]               0\n",
      "          Dropout-69            [-1, 8, 17, 17]               0\n",
      "           Linear-70              [-1, 17, 192]          37,056\n",
      "MultiheadSelfAtteintion-71              [-1, 17, 192]               0\n",
      "          Dropout-72              [-1, 17, 192]               0\n",
      "        LayerNorm-73              [-1, 17, 192]             384\n",
      "           Linear-74              [-1, 17, 768]         148,224\n",
      "             GELU-75              [-1, 17, 768]               0\n",
      "          Dropout-76              [-1, 17, 768]               0\n",
      "           Linear-77              [-1, 17, 192]         147,648\n",
      "      FeedForward-78              [-1, 17, 192]               0\n",
      "          Dropout-79              [-1, 17, 192]               0\n",
      "        LayerNorm-80              [-1, 17, 192]             384\n",
      "           Linear-81              [-1, 17, 576]         111,168\n",
      "        Rearrange-82           [-1, 2, 17, 192]               0\n",
      "        Rearrange-83         [-1, 2, 8, 17, 24]               0\n",
      "          Dropout-84            [-1, 8, 17, 17]               0\n",
      "           Linear-85              [-1, 17, 192]          37,056\n",
      "MultiheadSelfAtteintion-86              [-1, 17, 192]               0\n",
      "          Dropout-87              [-1, 17, 192]               0\n",
      "        LayerNorm-88              [-1, 17, 192]             384\n",
      "           Linear-89              [-1, 17, 768]         148,224\n",
      "             GELU-90              [-1, 17, 768]               0\n",
      "          Dropout-91              [-1, 17, 768]               0\n",
      "           Linear-92              [-1, 17, 192]         147,648\n",
      "      FeedForward-93              [-1, 17, 192]               0\n",
      "          Dropout-94              [-1, 17, 192]               0\n",
      "        LayerNorm-95              [-1, 17, 192]             384\n",
      "           Linear-96              [-1, 17, 576]         111,168\n",
      "        Rearrange-97           [-1, 2, 17, 192]               0\n",
      "        Rearrange-98         [-1, 2, 8, 17, 24]               0\n",
      "          Dropout-99            [-1, 8, 17, 17]               0\n",
      "          Linear-100              [-1, 17, 192]          37,056\n",
      "MultiheadSelfAtteintion-101              [-1, 17, 192]               0\n",
      "         Dropout-102              [-1, 17, 192]               0\n",
      "       LayerNorm-103              [-1, 17, 192]             384\n",
      "          Linear-104              [-1, 17, 768]         148,224\n",
      "            GELU-105              [-1, 17, 768]               0\n",
      "         Dropout-106              [-1, 17, 768]               0\n",
      "          Linear-107              [-1, 17, 192]         147,648\n",
      "     FeedForward-108              [-1, 17, 192]               0\n",
      "         Dropout-109              [-1, 17, 192]               0\n",
      "       LayerNorm-110              [-1, 17, 192]             384\n",
      "          Linear-111              [-1, 17, 576]         111,168\n",
      "       Rearrange-112           [-1, 2, 17, 192]               0\n",
      "       Rearrange-113         [-1, 2, 8, 17, 24]               0\n",
      "         Dropout-114            [-1, 8, 17, 17]               0\n",
      "          Linear-115              [-1, 17, 192]          37,056\n",
      "MultiheadSelfAtteintion-116              [-1, 17, 192]               0\n",
      "         Dropout-117              [-1, 17, 192]               0\n",
      "       LayerNorm-118              [-1, 17, 192]             384\n",
      "          Linear-119              [-1, 17, 768]         148,224\n",
      "            GELU-120              [-1, 17, 768]               0\n",
      "         Dropout-121              [-1, 17, 768]               0\n",
      "          Linear-122              [-1, 17, 192]         147,648\n",
      "     FeedForward-123              [-1, 17, 192]               0\n",
      "         Dropout-124              [-1, 17, 192]               0\n",
      "       LayerNorm-125                  [-1, 192]             384\n",
      "          Linear-126                   [-1, 10]           1,930\n",
      "================================================================\n",
      "Total params: 3,598,282\n",
      "Trainable params: 3,598,282\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 5.62\n",
      "Params size (MB): 13.73\n",
      "Estimated Total Size (MB): 19.36\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = VIT(image_size=(3,32,32), patch_size=8, num_classes = 10,vit_type='user' , embed_type = 'conv', embed_dropout=0.1,dropout=0.1)\n",
    "model(torch.randn(1,3,32,32))\n",
    "summary(model, (3,32,32),device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets,transforms\n",
    "from torchvision.transforms import ToTensor,Compose,Normalize\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_transform= Compose(\n",
    "    [\n",
    "     transforms.RandomHorizontalFlip(),\n",
    "     ToTensor(),\n",
    "     Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "     ])\n",
    "test_transform  = Compose(\n",
    "    [\n",
    "     transforms.ToTensor(),\n",
    "     Normalize((0.491, 0.482 ,0.447), (0.247, 0.243, 0.262))\n",
    "     ])\n",
    "train_set = datasets.CIFAR10(root='./data/',train=True,download=True,transform = train_transform)\n",
    "test_set = datasets.CIFAR10(root='./data/',train=False,download=True,transform = test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 \n",
    "EPOCHS = 10\n",
    "LR = 1e-3\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = LR ,weight_decay=0.01)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max= EPOCHS, eta_min=LR/20)\n",
    "train_loader = DataLoader(train_set, BATCH_SIZE)\n",
    "test_loader = DataLoader(test_set, BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visualai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
